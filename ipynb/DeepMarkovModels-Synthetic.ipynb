{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook walks through creating, setting up and training a Deep Markov Model on a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General Purpose Imports\n",
    "import numpy as np\n",
    "import glob, os, sys, time\n",
    "sys.path.append('../')\n",
    "from utils.misc import getConfigFile, readPickle, displayTime\n",
    "\n",
    "#Matplotlib imports\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth']=2.5\n",
    "mpl.rcParams['lines.markersize']=8\n",
    "mpl.rcParams['text.usetex']=True\n",
    "mpl.rcParams['text.latex.unicode']=True\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'Times New Roman'\n",
    "mpl.rcParams['text.latex.preamble']= ['\\usepackage{amsfonts}','\\usepackage{amsmath}']\n",
    "mpl.rcParams['font.size'] = 20\n",
    "mpl.rcParams['axes.labelsize']=20\n",
    "mpl.rcParams['legend.fontsize']=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A] Data\n",
    "* The polyphonic music code (which can be run from the `expt` folder) represents an example of running on binary data\n",
    "* There is code to create a synthetic dataset in `dmm_data/load.py`, we will load that dataset here (it will be created the first time this function is called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading linear matrices\n",
      "Loading linear matrices\n",
      "Reloading...\n",
      "Read  1  objects\n",
      "<type 'dict'> ['test', 'dim_observations', 'train', 'valid', 'data_type']\n"
     ]
    }
   ],
   "source": [
    "#Import load function to load synthetic data\n",
    "from dmm_data.load import load\n",
    "dataset = load('synthetic')\n",
    "print type(dataset), dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Dataset format\n",
    "The dataset's are expected to be in a specific format:\n",
    "* The type of the variable `dataset` is `dict`\n",
    "* The keys of this dataset correspond to parameters used by the model as well as raw data\n",
    "* `dim_observations` is of type `int` and it corresponds to the dimensionality of the raw data at each point in time\n",
    "* `data_type` can be `binary` or `real`\n",
    "* `train`,`valid` and `test` are dictionaries that house the pre-split data. `dataset['train']` contains three items: \n",
    "    * `dataset['train']['tensor']` is a 3D tensor where the dimensions correspond to `Nsamples x T x dim_observations`\n",
    "    * `dataset['train']['mask']` is a 2D matrix where the dimensions correspond to `Nsamples x T`. Each entry in this matrix corresponds to whether a sample was observed at that point in time. This is used internally by the model to model variable length sequences. \n",
    "    * `dataset['train']['tensor_Z']` is not used by the model. It corresponds to the states of the **true** underlying latent variables that created this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the observations:  3\n",
      "Data type of features: real\n",
      "dtype:  train  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (10000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (10000, 10)), ('tensor', <type 'numpy.ndarray'>, (10000, 10, 3))]\n",
      "--------\n",
      "\n",
      "dtype:  valid  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (1000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 3))]\n",
      "--------\n",
      "\n",
      "dtype:  test  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (1000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 3))]\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Dimensionality of the observations: ', dataset['dim_observations']\n",
    "print 'Data type of features:', dataset['data_type']\n",
    "for dtype in ['train','valid','test']:\n",
    "    print 'dtype: ',dtype, ' type(dataset[dtype]): ',type(dataset[dtype])\n",
    "    print [(k,type(dataset[dtype][k]), dataset[dtype][k].shape) for k in dataset[dtype]]\n",
    "    print '--------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Creating your own dataset\n",
    "* When creating your own data, it should have the following structure: \n",
    "    * type: dict\n",
    "        * `dim_observations`: int\n",
    "        * `data_type`: `binary` or `real`\n",
    "        * `train`, `valid`, `test` must be dictionaries with the following keys: \n",
    "            * `tensor` : Raw data as a 3D tensor with dimensions `Nsamples x T x dim_observations`\n",
    "            * `mask`   : Mask for the raw data as a 2D matrix with dimensions `Nsamples x T`\n",
    "* Now that we have the dataset in the desired format, lets look at setting up the model, we'll first load the necessary files to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t< importing DMM > took  1.51789593697   seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from   model_th.dmm import DMM\n",
    "import model_th.learning as DMM_learn\n",
    "import model_th.evaluate as DMM_evaluate\n",
    "displayTime('importing DMM',start_time, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## B] Model Hyperparameters\n",
    "* To setup the model, we need to specify the hyperparameters\n",
    "* Normally, if you were running from a script, you would run the following: `from parse_args import params` (e.g `expt/train.py` within the script.\n",
    "* This lets you specify hyperparameters for the model via the command line. See the shell scripts in the folder `expt/` for an example of this. \n",
    "* Since we're in Ipython, we'll reload a saved version of `params` and see what the default values currently are. To know more about how the choices of hyperparameters affect the model, you can run `python parse_args.py -h` in the main directory.\n",
    "* The `unique_id` is created based on the default parameters or those specified via the commend line.\n",
    "* The parameters (`data_type` and `dim_observations`) of the dataset need to be incorporated into `params` for the model to be able to setup the weights appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read  1  objects\n",
      "dataset \tmm\n",
      "epochs \t2000\n",
      "seed \t1\n",
      "init_weight \t0.1\n",
      "dim_stochastic \t100\n",
      "expt_name \tuid\n",
      "reg_value \t0.05\n",
      "reloadFile \t./NOSUCHFILE\n",
      "reg_spec \t_\n",
      "dim_hidden \t200\n",
      "lr \t0.0008\n",
      "reg_type \tl2\n",
      "init_scheme \tuniform\n",
      "optimizer \tadam\n",
      "use_generative_prior \tapprox\n",
      "maxout_stride \t4\n",
      "batch_size \t20\n",
      "savedir \t./chkpt\n",
      "forget_bias \t-5.0\n",
      "inference_model \tR\n",
      "emission_layers \t2\n",
      "savefreq \t10\n",
      "rnn_cell \tlstm\n",
      "rnn_size \t600\n",
      "paramFile \t./NOSUCHFILE\n",
      "nonlinearity \trelu\n",
      "rnn_dropout \t0.1\n",
      "transition_layers \t2\n",
      "anneal_rate \t2.0\n",
      "debug \tFalse\n",
      "validate_only \tFalse\n",
      "transition_type \tmlp\n",
      "unique_id \tDMM_lr-0_0008-dh-200-ds-100-nl-relu-bs-20-ep-2000-rs-600-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid\n",
      "leaky_param \t0.0\n"
     ]
    }
   ],
   "source": [
    "params = readPickle('../default.pkl')[0]\n",
    "for k in params:\n",
    "    print k, '\\t',params[k]\n",
    "params['data_type'] = dataset['data_type']\n",
    "params['dim_observations'] = dataset['dim_observations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C] Building the DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* Thats it! An abridged version of this code is available for use in `expt-template` for easier modification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

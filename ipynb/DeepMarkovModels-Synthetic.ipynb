{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook walks through creating, setting up and training a Deep Markov Model on a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General Purpose Imports\n",
    "import numpy as np\n",
    "import glob, os, sys, time\n",
    "sys.path.append('../')\n",
    "from utils.misc import getConfigFile, readPickle, displayTime\n",
    "\n",
    "#Matplotlib imports\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth']=2.5\n",
    "mpl.rcParams['lines.markersize']=8\n",
    "mpl.rcParams['text.usetex']=True\n",
    "mpl.rcParams['text.latex.unicode']=True\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'Times New Roman'\n",
    "mpl.rcParams['text.latex.preamble']= ['\\usepackage{amsfonts}','\\usepackage{amsmath}']\n",
    "mpl.rcParams['font.size'] = 20\n",
    "mpl.rcParams['axes.labelsize']=20\n",
    "mpl.rcParams['legend.fontsize']=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A] Data\n",
    "* The polyphonic music code (which can be run from the `expt` folder) represents an example of running on binary data\n",
    "* There is code to create a synthetic dataset in `dmm_data/load.py`, we will load that dataset here (it will be created the first time this function is called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading linear matrices\n",
      "Loading linear matrices\n",
      "Reloading...\n",
      "Read  1  objects\n",
      "<type 'dict'> ['test', 'dim_observations', 'train', 'valid', 'data_type']\n"
     ]
    }
   ],
   "source": [
    "#Import load function to load synthetic data\n",
    "from dmm_data.load import load\n",
    "dataset = load('synthetic')\n",
    "print type(dataset), dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Dataset format\n",
    "The dataset's are expected to be in a specific format:\n",
    "* The type of the variable `dataset` is `dict`\n",
    "* The keys of this dataset correspond to parameters used by the model as well as raw data\n",
    "* `dim_observations` is of type `int` and it corresponds to the dimensionality of the raw data at each point in time\n",
    "* `data_type` can be `binary` or `real`\n",
    "* `train`,`valid` and `test` are dictionaries that house the pre-split data. `dataset['train']` contains three items: \n",
    "    * `dataset['train']['tensor']` is a 3D tensor where the dimensions correspond to `Nsamples x T x dim_observations`\n",
    "    * `dataset['train']['mask']` is a 2D matrix where the dimensions correspond to `Nsamples x T`. Each entry in this matrix corresponds to whether a sample was observed at that point in time. This is used internally by the model to model variable length sequences. \n",
    "    * `dataset['train']['tensor_Z']` is not used by the model. It corresponds to the states of the **true** underlying latent variables that created this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the observations:  3\n",
      "Data type of features: real\n",
      "dtype:  train  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (10000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (10000, 10)), ('tensor', <type 'numpy.ndarray'>, (10000, 10, 3))]\n",
      "--------\n",
      "\n",
      "dtype:  valid  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (1000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 3))]\n",
      "--------\n",
      "\n",
      "dtype:  test  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (1000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 3))]\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Dimensionality of the observations: ', dataset['dim_observations']\n",
    "print 'Data type of features:', dataset['data_type']\n",
    "for dtype in ['train','valid','test']:\n",
    "    print 'dtype: ',dtype, ' type(dataset[dtype]): ',type(dataset[dtype])\n",
    "    print [(k,type(dataset[dtype][k]), dataset[dtype][k].shape) for k in dataset[dtype]]\n",
    "    print '--------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Creating your own dataset\n",
    "* When creating your own data, it should have the following structure: \n",
    "    * type: dict\n",
    "        * `dim_observations`: int\n",
    "        * `data_type`: `binary` or `real`\n",
    "        * `train`, `valid`, `test` must be dictionaries with the following keys: \n",
    "            * `tensor` : Raw data as a 3D tensor with dimensions `Nsamples x T x dim_observations`\n",
    "            * `mask`   : Mask for the raw data as a 2D matrix with dimensions `Nsamples x T`\n",
    "* Now that we have the dataset in the desired format, lets look at setting up the model, we'll first load the necessary files to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t< importing DMM > took  3.5654168129   seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from   model_th.dmm import DMM\n",
    "import model_th.learning as DMM_learn\n",
    "import model_th.evaluate as DMM_evaluate\n",
    "displayTime('importing DMM',start_time, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## B] Model Hyperparameters\n",
    "* To setup the model, we need to specify the hyperparameters\n",
    "* Normally, if you were running from a script, you would run the following: `from parse_args import params` (e.g `expt/train.py` within the script.\n",
    "* This lets you specify hyperparameters for the model via the command line. See the shell scripts in the folder `expt/` for an example of this. \n",
    "* Since we're in Ipython, we'll reload a saved version of `params` and see what the default values currently are. To know more about how the choices of hyperparameters affect the model, you can run `python parse_args.py -h` in the main directory.\n",
    "* The `unique_id` is created based on the default parameters or those specified via the commend line.\n",
    "* The parameters (`data_type` and `dim_observations`) of the dataset need to be incorporated into `params` for the model to be able to setup the weights appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read  1  objects\n",
      "dataset \tmm\n",
      "epochs \t2000\n",
      "seed \t1\n",
      "init_weight \t0.1\n",
      "dim_stochastic \t100\n",
      "expt_name \tuid\n",
      "reg_value \t0.05\n",
      "reloadFile \t./NOSUCHFILE\n",
      "reg_spec \t_\n",
      "dim_hidden \t200\n",
      "lr \t0.0008\n",
      "reg_type \tl2\n",
      "init_scheme \tuniform\n",
      "optimizer \tadam\n",
      "use_generative_prior \tapprox\n",
      "maxout_stride \t4\n",
      "batch_size \t20\n",
      "savedir \t./chkpt\n",
      "forget_bias \t-5.0\n",
      "inference_model \tR\n",
      "emission_layers \t2\n",
      "savefreq \t10\n",
      "rnn_cell \tlstm\n",
      "rnn_size \t600\n",
      "paramFile \t./NOSUCHFILE\n",
      "nonlinearity \trelu\n",
      "rnn_dropout \t0.1\n",
      "transition_layers \t2\n",
      "anneal_rate \t2.0\n",
      "debug \tFalse\n",
      "validate_only \tFalse\n",
      "transition_type \tmlp\n",
      "unique_id \tDMM_lr-0_0008-dh-200-ds-100-nl-relu-bs-20-ep-2000-rs-600-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid\n",
      "leaky_param \t0.0\n"
     ]
    }
   ],
   "source": [
    "params = readPickle('../default.pkl')[0]\n",
    "for k in params:\n",
    "    print k, '\\t',params[k]\n",
    "params['data_type'] = dataset['data_type']\n",
    "params['dim_observations'] = dataset['dim_observations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C] Building the DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint prefix:  ./chkpt-ipython//DMM_lr-0_0008-dh-40-ds-2-nl-relu-bs-200-ep-10-rs-80-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-config.pkl\n",
      "\t<<Sampling biases for LSTM from exponential distribution>>\n",
      "\t<<Nparameters: 56334>>\n",
      "\t<<WARNING: lr will not differentiated with respect to>>\n",
      "\t<<WARNING: anneal will not differentiated with respect to>>\n",
      "\t<<WARNING: update_ctr will not differentiated with respect to>>\n",
      "\t<<Anneal = 1 in 2.0 param. updates>>\n",
      "\t<<Building with RNN dropout:0.1>>\n",
      "\t<<In _LSTM_RNN_layer with dropout 0.1000>>\n",
      "\t<<Modifying : [q_W_input_0,q_b_input_0,W_lstm_r,b_lstm_r,U_lstm_r,q_W_st,q_b_st,q_W_mu,q_b_mu,q_W_cov,q_b_cov,p_trans_W_0,p_trans_b_0,p_trans_W_1,p_trans_b_1,p_trans_W_mu,p_trans_b_mu,p_trans_W_cov,p_trans_b_cov,p_emis_W_0,p_emis_b_0,p_emis_W_1,p_emis_b_1,p_emis_W_out,p_emis_b_out]>>\n",
      "<< Reg:(l2) Reg. Val:(0.05) Reg. Spec.:(_)>>\n",
      "<<<<<< Adding l2 regularization for q_W_input_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_input_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for W_lstm_r >>>>>>\n",
      "<<<<<< Adding l2 regularization for b_lstm_r >>>>>>\n",
      "<<<<<< Adding l2 regularization for U_lstm_r >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_W_st >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_st >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_W_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_W_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_W_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_b_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_W_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_b_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_W_out >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_b_out >>>>>>\n",
      "<<<<<< Normalizing Gradients to have norm ( 1.0 ) >>>>>>\n",
      "\t<<0 other updates>>\n",
      "\t<<Building with RNN dropout:0.0>>\n",
      "\t<<In _LSTM_RNN_layer with dropout 0.0000>>\n",
      "\t<<Completed DMM setup>>\n",
      "\t<<_buildModel took : 7.8792 seconds>>\n"
     ]
    }
   ],
   "source": [
    "#The dataset is small, lets change some of the default parameters and the unique ID\n",
    "params['dim_stochastic'] = 2\n",
    "params['dim_hidden']     = 40\n",
    "params['rnn_size']       = 80\n",
    "params['epochs']         = 10\n",
    "params['batch_size']     = 200\n",
    "params['unique_id'] = params['unique_id'].replace('ds-100','ds-2').replace('dh-200','dh-40').replace('rs-600','rs-80')\n",
    "params['unique_id'] = params['unique_id'].replace('ep-2000','ep-10').replace('bs-20','bs-200')\n",
    "\n",
    "#Create a temporary directory to save checkpoints\n",
    "params['savedir']   = params['savedir']+'-ipython/'\n",
    "os.system('mkdir -p '+params['savedir'])\n",
    "\n",
    "#Specify the file where `params` corresponding for this choice of model and data will be saved\n",
    "pfile= params['savedir']+'/'+params['unique_id']+'-config.pkl'\n",
    "\n",
    "print 'Checkpoint prefix: ', pfile\n",
    "dmm  = DMM(params, paramFile = pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D] Parameter Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<<Original dim: [3 5 3],[3 5]>>\n",
      "\t<<New dim: [10000    10     3],[10000    10]>>\n",
      "\t<<Bnum: 0, Batch Bound: 27.2308, |w|: 34.3823, |dw|: 1.0000, |w_opt|: 0.0000>>\n",
      "\t<<-veCLL:54461.2360, KL:34.9538, anneal:0.0100>>\n",
      "\t<<Bnum: 10, Batch Bound: 26.6092, |w|: 34.0214, |dw|: 1.0000, |w_opt|: 0.6507>>\n",
      "\t<<-veCLL:53199.3876, KL:19.0197, anneal:1.0000>>\n",
      "\t<<Bnum: 20, Batch Bound: 26.2080, |w|: 33.6336, |dw|: 1.0000, |w_opt|: 0.8759>>\n",
      "\t<<-veCLL:52388.2278, KL:27.8222, anneal:1.0000>>\n",
      "\t<<Bnum: 30, Batch Bound: 25.5067, |w|: 33.2729, |dw|: 1.0000, |w_opt|: 0.9516>>\n",
      "\t<<-veCLL:50922.5333, KL:90.9376, anneal:1.0000>>\n",
      "\t<<Bnum: 40, Batch Bound: 23.4843, |w|: 33.0148, |dw|: 1.0000, |w_opt|: 0.9758>>\n",
      "\t<<-veCLL:46639.7084, KL:328.8080, anneal:1.0000>>\n",
      "\t<<(Ep 0) Bound: 25.3746 [Took 28.9434 seconds] >>\n",
      "\t<<Saving at epoch 0>>\n",
      "\t<<Saved model (./chkpt-ipython/DMM_lr-0_0008-dh-40-ds-2-nl-relu-bs-200-ep-10-rs-80-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP0-params) \n",
      "\t\t opt (./chkpt-ipython/DMM_lr-0_0008-dh-40-ds-2-nl-relu-bs-200-ep-10-rs-80-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP0-optParams) weights>>\n",
      "\t<<Original dim: [10000    10     3],[10000    10]>>\n",
      "\t<<New dim: [1000   10    3],[1000   10]>>\n",
      "\t<<(Evaluate) Validation Bound: 21.7737 [Took 0.3498 seconds]>>\n",
      "\t<<Original dim: [1000   10    3],[1000   10]>>\n",
      "\t<<New dim: [10000    10     3],[10000    10]>>\n",
      "\t<<Bnum: 0, Batch Bound: 21.7500, |w|: 32.8990, |dw|: 1.0000, |w_opt|: 0.9779>>\n",
      "\t<<-veCLL:42498.3666, KL:1001.6501, anneal:1.0000>>\n",
      "\t<<Bnum: 10, Batch Bound: 18.1202, |w|: 32.9593, |dw|: 1.0000, |w_opt|: 0.9700>>\n",
      "\t<<-veCLL:34058.2622, KL:2182.1279, anneal:1.0000>>\n",
      "\t<<Bnum: 20, Batch Bound: 13.1453, |w|: 33.1640, |dw|: 1.0000, |w_opt|: 0.9648>>\n",
      "\t<<-veCLL:23166.1868, KL:3124.3364, anneal:1.0000>>\n",
      "\t<<Bnum: 30, Batch Bound: 9.8278, |w|: 33.3261, |dw|: 1.0000, |w_opt|: 0.9586>>\n",
      "\t<<-veCLL:17548.0917, KL:2107.5034, anneal:1.0000>>\n",
      "\t<<Bnum: 40, Batch Bound: 9.0338, |w|: 33.1978, |dw|: 1.0000, |w_opt|: 0.8181>>\n",
      "\t<<-veCLL:17074.5892, KL:992.9523, anneal:1.0000>>\n",
      "\t<<(Ep 1) Bound: 13.1309 [Took 30.3898 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 8.8456, |w|: 32.8695, |dw|: 1.0000, |w_opt|: 0.6439>>\n",
      "\t<<-veCLL:16932.1652, KL:758.9745, anneal:1.0000>>\n",
      "\t<<Bnum: 10, Batch Bound: 8.7399, |w|: 32.5070, |dw|: 1.0000, |w_opt|: 0.7350>>\n",
      "\t<<-veCLL:16776.5999, KL:703.2793, anneal:1.0000>>\n",
      "\t<<Bnum: 20, Batch Bound: 8.6359, |w|: 32.1860, |dw|: 1.0000, |w_opt|: 0.7429>>\n",
      "\t<<-veCLL:16683.0759, KL:588.6587, anneal:1.0000>>\n",
      "\t<<Bnum: 30, Batch Bound: 8.5780, |w|: 31.8969, |dw|: 1.0000, |w_opt|: 0.7538>>\n",
      "\t<<-veCLL:16678.5735, KL:477.5127, anneal:1.0000>>\n",
      "\t<<Bnum: 40, Batch Bound: 8.4976, |w|: 31.6158, |dw|: 1.0000, |w_opt|: 0.7143>>\n",
      "\t<<-veCLL:16605.9090, KL:389.2594, anneal:1.0000>>\n",
      "\t<<(Ep 2) Bound: 8.6254 [Took 30.5761 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 8.5327, |w|: 31.3255, |dw|: 1.0000, |w_opt|: 0.7329>>\n",
      "\t<<-veCLL:16791.0065, KL:274.3846, anneal:1.0000>>\n",
      "\t<<Bnum: 10, Batch Bound: 8.4751, |w|: 31.0670, |dw|: 1.0000, |w_opt|: 0.6509>>\n",
      "\t<<-veCLL:16755.2574, KL:194.8431, anneal:1.0000>>\n",
      "\t<<Bnum: 20, Batch Bound: 8.4569, |w|: 30.7898, |dw|: 1.0000, |w_opt|: 0.6146>>\n",
      "\t<<-veCLL:16803.4309, KL:110.2961, anneal:1.0000>>\n",
      "\t<<Bnum: 30, Batch Bound: 8.4527, |w|: 30.5088, |dw|: 1.0000, |w_opt|: 0.4502>>\n",
      "\t<<-veCLL:16841.0682, KL:64.3525, anneal:1.0000>>\n",
      "\t<<Bnum: 40, Batch Bound: 8.3888, |w|: 30.2443, |dw|: 1.0000, |w_opt|: 0.2624>>\n",
      "\t<<-veCLL:16739.0412, KL:38.5805, anneal:1.0000>>\n",
      "\t<<(Ep 3) Bound: 8.4458 [Took 29.9976 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 8.4654, |w|: 29.9851, |dw|: 1.0000, |w_opt|: 0.2495>>\n",
      "\t<<-veCLL:16897.9594, KL:32.8898, anneal:1.0000>>\n"
     ]
    }
   ],
   "source": [
    "#savef specifies the prefix for the checkpoints - we'll use the same save directory as before \n",
    "savef    = os.path.join(params['savedir'],params['unique_id'])\n",
    "savedata = DMM_learn.learn(dmm, dataset['train'], epoch_start =0 ,\n",
    "                                epoch_end = 100,\n",
    "                                batch_size = 200,\n",
    "                                savefreq   = params['savefreq'],\n",
    "                                savefile   = savef,\n",
    "                                dataset_eval=dataset['valid'],\n",
    "                                shuffle    = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E] Checking the Learned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F] Reloading a Saved Model\n",
    "If the model is overfitting, you may want to reload from a saved checkpoint - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* Thats it! An abridged version of this code is available for use in `expt-template` for easier modification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
